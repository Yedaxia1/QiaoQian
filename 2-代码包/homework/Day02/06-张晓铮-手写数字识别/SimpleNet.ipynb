{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义SimpleNet结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplenet(nn.Module):\n",
    "    def __init__(self, classes=10, simpnet_name='simplenet'):\n",
    "        super(simplenet, self).__init__()\n",
    "        #print(simpnet_name)\n",
    "        self.features = self._make_layers() #self._make_layers(cfg[simpnet_name])\n",
    "        self.classifier = nn.Linear(256, classes)\n",
    "        self.drp = nn.Dropout(0.1)\n",
    "\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "\n",
    "        # print(own_state.keys())\n",
    "        # for name, val in own_state:\n",
    "        # print(name)\n",
    "        for name, param in state_dict.items():\n",
    "            name = name.replace('module.', '')\n",
    "            if name not in own_state:\n",
    "                # print(name)\n",
    "                continue\n",
    "            if isinstance(param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            print(\"STATE_DICT: {}\".format(name))\n",
    "            try:\n",
    "                own_state[name].copy_(param)\n",
    "            except:\n",
    "                print('While copying the parameter named {}, whose dimensions in the model are'\n",
    "                      ' {} and whose dimensions in the checkpoint are {}, ... Using Initial Params'.format(\n",
    "                    name, own_state[name].size(), param.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "\n",
    "        #Global Max Pooling\n",
    "        out = F.max_pool2d(out, kernel_size=out.size()[2:]) \n",
    "        # out = F.dropout2d(out, 0.1, training=True)\n",
    "        out = self.drp(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self):\n",
    "\n",
    "        model = nn.Sequential(\n",
    "                             nn.Conv2d(1, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(64, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(512, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
    "                             nn.BatchNorm2d(2048, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.Conv2d(2048, 256, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                            )\n",
    "\n",
    "        for m in model.modules():\n",
    "          if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.Resize((32, 32)),                           \n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize(\n",
    "                                         (0.1307,), (0.3081,))\n",
    "                                     ]))\n",
    "test_set = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.Resize((32, 32)),                           \n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize(\n",
    "                                         (0.1307,), (0.3081,))\n",
    "                                     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 540\n",
    "batch_size = 100\n",
    "num_workers = 2\n",
    "save_path = './checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simplenet()\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(net.parameters(), lr=0.1, rho=0.9, eps=1e-3, weight_decay=0.001)\n",
    "\n",
    "milestones = [100, 190, 306, 390, 440, 540]\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to('cuda')\n",
    "loss = loss.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, data_iter):\n",
    "    device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X.to(device)).argmax(dim= 1) == y.to(device)).float().sum().cpu().item()\n",
    "        n += y.shape[0]\n",
    "    net.train()\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test accuracy 0.802700\n",
      "epoch 2 test accuracy 0.992800\n",
      "epoch 3 test accuracy 0.993800\n",
      "epoch 4 test accuracy 0.994200\n",
      "epoch 5 test accuracy 0.994500\n",
      "epoch 6 test accuracy 0.995200\n",
      "epoch 7 test accuracy 0.995200\n",
      "epoch 8 test accuracy 0.995700\n",
      "epoch 9 test accuracy 0.994500\n",
      "epoch 10 test accuracy 0.996600\n",
      "epoch 11 test accuracy 0.994700\n",
      "epoch 12 test accuracy 0.991800\n",
      "epoch 13 test accuracy 0.996100\n",
      "epoch 14 test accuracy 0.994000\n",
      "epoch 15 test accuracy 0.996300\n",
      "epoch 16 test accuracy 0.994400\n",
      "epoch 17 test accuracy 0.989500\n",
      "epoch 18 test accuracy 0.996000\n",
      "epoch 19 test accuracy 0.992300\n",
      "epoch 20 test accuracy 0.995400\n",
      "epoch 21 test accuracy 0.995700\n",
      "epoch 22 test accuracy 0.993900\n"
     ]
    }
   ],
   "source": [
    "best_acc = -1\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        X = X.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        l = loss(net(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    acc = evaluate(net, test_loader)\n",
    "    print('epoch %d test accuracy %f' % (epoch + 1, acc))\n",
    "          \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(), save_path + 'checkpoints_epoch_{}.pth'.format(epoch + 1))\n",
    "    if acc > best_acc:\n",
    "        torch.save(net.state_dict(), save_path + 'best_checkpoints_acc_{}.pth'.format(acc))\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
